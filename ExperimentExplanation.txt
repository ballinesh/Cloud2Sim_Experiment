--->,<--- indicate what I used to create further simulations

All data textfiles can be found in the repository and they are generated by the code
I purposly made the number of cloudlets limited as to not make huge filesizes for and for the sake of time
but feel free to play around with the values

My goal for this project was to see what is the most optimal VM/Host setup that will get the job done fastests
with the same resources. 

I conducted four different Sims to gather data on how it all works. The configuration I used for the simulator
involved only one VM can work on one cloudlet at a time. I did this because otherwise resources were just being
allocated regardless of the setup I wanted and it gave me more freedom to experiment

Resources used: Datacenter1: 4vm 1000 mips
		Datacenter2: 8vm 500  mips
		Datacenter3: 2vm 2000 mips

Workloads Used: Sim1: 10 	cloudlets, 50000  Mips
	   	Sim2: 1000	cloudlets, 50000  Mips
	   	Sim3: 10 	cloudlets, 100	  Mips
	   	Sim4: 1000   	cloudlets, 100	  Mips
		Sim5: 18	cloudlets, Mix    50, 100, 200
		Sim6: Random	cloudlets, Random
		Sim7: 1		cloudlet,  100000 Mips
		Sim8: 1		cloudlet,  Random

Sim1 Findings: ---->I found that one way to optimize scheduling is that the number of jobs to be done must be a multiple 
		of the number of VM's.In this case the fastest was the 2VM host because there were no resources
		being wasted. If you look at the data for the first and second VM Lists we can see that for every 
		VM not being used during the second round we lose that much number of time. <----
		
		To put it into numbers, Datacenter 1 could be doing 12 cloudlets just as fast as it would do 10
		so allocating 4VMs to a 10 cloudlet job we are wasting resources and customers money!


Sim2 Findings: My findings of Sim1 were confirmed even with many cloudlets as long as the number of cloudlets is
		a multiple then we are using our resources efficiently. Here again it may be more cost effective to
		have lets say just two very fast machines allocated vs 8 slower ones. The 8 slower once will 
		cost more because of the extra hosts it has to power up


Sim3 Findings: Things continue to get interesting. I found that the 4vm Datacenter was much faster than both of the
		other choices. This is because after a certain speed for a small task starts to have no benefit
		the weaker vm of 1000mips finished the task in the same time as the 2000mips vm. If we know the workload
		we can continue to save money by maybe instead provisioning more weaker vms instead of stronger ones
		--->I also found that the ratio where we get no more benefit is around 10:1<----
			10 being processing power and 1 being work to process
	
Sim4 Findings: This sim just confirms what I found for Sim3, Having more vms in this situation however was slightly better
		because the speed was not two times slower when going from 4Vms to 8vm with half the mips

--------------------Where I applied my information--------------------

	---Sim5---
So what do I do with this knowledge? Well in the 5th sim I built a network of datacenters and vms that work together. 
In here I used my knowledge to properly allocate my workload. For this one I just made a simple experiment to test my algorithm
I simply made a list of cloudlets and gave them each a workload that SHOULD correspond to the different types of Vm's in the datacenter.
You can see the algorithm in the function "smartSubmitCloudletList" I grabbed the ratio of processing power to workload and submitted a cloudlet
only if the ratio was smaller than 10, if the ratio is greater than 10 then we are wasting time and resources and therefore the customers money!

For the algorithm to work we need to first check the strongest vm's and then finish with the weakest in terms of MIP processing. Again you can 
walk through the logic in main and in the function I mentioned earlier but the reason behind this is that the ratio for weaker vm's will be less than 10
on harder workloads meaning that workloads will be incorrectly allocated here.

For this controlled example you can see that no time is being wasted and the process finishes in 0.32 to process 16 cloudlets

	---Sim6---
Sim 6 is the same as sim 5 but with random values and number of cloudlets. This one is not as well optimized since it is not a perfect situation
but I think it still does a great job at producing good results

	---Sim7---
This is where my implementation of Map/reduce is. The main idea here is that map/reduce is supposed to take a big job and split it up among different vm's
to speed up the job significantly. 

For my implementation we assume that we already know how many VM's are at our disposal and which types (How much they can process). We will also use our knowledge from the first four sims
to create the algorithm for the map and reduce function. 

1.Using multiples to our advantage.
	-In order to not waste any resources we need to split up the cloudlet into a number of cloudlets where a VM will always be running on it.
	-What I did here was divide the cloudlet into three sections. One for each datacenter. I divided the MIPs evenly.
	-Then on one third for example, I would keep dividing it only by the number of VM's that I want to assign to that third
	-This would ensure that no VM is left without a job and no resources are being wasted

2. Using the magic 10:1 ratio
	-Each third is grouped by the MIPS of a set of VM's
	-The loop above stops when a ratio of Mips on vm is less than MIPS on Cloudlets.

From doing these two things I got pretty decent results. The only problem was that something it created a little more number of tasks that may not have been worth it. However after sticking with it 
this is obviously still better than not doing any reduction at all

	---Sim8---
Sim8 is the same as sim7 but we are using a random workload

------------Conclusion---------------
I found these interesting things that I'm sure all cloud providers already know about but it was very fun to discover for myself. Using the information to design my own simulations was actually pretty 
rewarding. Although I didn't make the best Map/reduce algorithm I'm glad I was able to use my knowledge to produce something better than the default.


